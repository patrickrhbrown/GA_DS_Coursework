{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Guided Practice with Topic Modeling and LDA\n",
    "\n",
    "---\n",
    "\n",
    "> **Note: this lab is intended to be a guided lab with the instructor.**\n",
    "\n",
    "In practice it would be a very rare to need to build an unsupervised topic model like LDA from scratch. Lucky for us, sklearn comes with LDA topic modeling functionality. Another popular LDA module which we will explore in this lab is from the `gensim` package. \n",
    "\n",
    "Let's explore a brief walkthrough of LDA and topic modeling using gensim. We will work with a small collection of documents represented as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the packages and create the small \"documents\".\n",
    "\n",
    "You may need to install the gensim package with `pip` or `conda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "documents = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "df        = pd.DataFrame(documents, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brocolli is good to eat. My brother likes to e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mother spends a lot of time driving my brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some health experts suggest that driving may c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I often feel pressure to perform well at schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Health professionals say that brocolli is good...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Brocolli is good to eat. My brother likes to e...\n",
       "1  My mother spends a lot of time driving my brot...\n",
       "2  Some health experts suggest that driving may c...\n",
       "3  I often feel pressure to perform well at schoo...\n",
       "4  Health professionals say that brocolli is good..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load stop words either from NLTK or sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stops = stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "custom_stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# You can of course add your own custom stopwords\n",
    "custom_stop_words.append('mother')\n",
    "custom_stop_words.append('brother')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use CountVectorizer to transform our text, taking out the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
    "X = vectorizer.fit_transform(df['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract the tokens that remain after stopword removal.\n",
    "\n",
    "The `.vocabulary_` attribute of the vectorizer contains a dictionary of terms. There is also the built-in function `.get_feature_names()` which will extract the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'baseball': 0,\n",
       " u'better': 1,\n",
       " u'blood': 2,\n",
       " u'brocolli': 3,\n",
       " u'cause': 4,\n",
       " u'drive': 5,\n",
       " u'driving': 6,\n",
       " u'eat': 7,\n",
       " u'experts': 8,\n",
       " u'feel': 9,\n",
       " u'good': 10,\n",
       " u'health': 11,\n",
       " u'increased': 12,\n",
       " u'likes': 13,\n",
       " u'lot': 14,\n",
       " u'perform': 15,\n",
       " u'practice': 16,\n",
       " u'pressure': 17,\n",
       " u'professionals': 18,\n",
       " u'say': 19,\n",
       " u'school': 20,\n",
       " u'spends': 21,\n",
       " u'suggest': 22,\n",
       " u'tension': 23,\n",
       " u'time': 24}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'baseball',\n",
       " u'better',\n",
       " u'blood',\n",
       " u'brocolli',\n",
       " u'cause',\n",
       " u'drive',\n",
       " u'driving',\n",
       " u'eat',\n",
       " u'experts',\n",
       " u'feel',\n",
       " u'good',\n",
       " u'health',\n",
       " u'increased',\n",
       " u'likes',\n",
       " u'lot',\n",
       " u'perform',\n",
       " u'practice',\n",
       " u'pressure',\n",
       " u'professionals',\n",
       " u'say',\n",
       " u'school',\n",
       " u'spends',\n",
       " u'suggest',\n",
       " u'tension',\n",
       " u'time']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get counts of tokens.\n",
    "\n",
    "Convert the matrix from the vectorizer to a dense matrix, then sum by column to get the counts per term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baseball         1\n",
       "better           1\n",
       "blood            1\n",
       "brocolli         3\n",
       "cause            1\n",
       "drive            1\n",
       "driving          2\n",
       "eat              2\n",
       "experts          1\n",
       "feel             1\n",
       "good             3\n",
       "health           3\n",
       "increased        1\n",
       "likes            1\n",
       "lot              1\n",
       "perform          1\n",
       "practice         1\n",
       "pressure         2\n",
       "professionals    1\n",
       "say              1\n",
       "school           1\n",
       "spends           1\n",
       "suggest          1\n",
       "tension          1\n",
       "time             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "docs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Setup the vocabulary dictionary\n",
    "\n",
    "First we need to setup the vocabulary.  Gensim's LDA expects our vocabulary to be in a format where the dictionary keys are the column indices and the values are the words themselves.\n",
    "\n",
    "Create this dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'baseball',\n",
       " 1: u'better',\n",
       " 2: u'blood',\n",
       " 3: u'brocolli',\n",
       " 4: u'cause',\n",
       " 5: u'drive',\n",
       " 6: u'driving',\n",
       " 7: u'eat',\n",
       " 8: u'experts',\n",
       " 9: u'feel',\n",
       " 10: u'good',\n",
       " 11: u'health',\n",
       " 12: u'increased',\n",
       " 13: u'likes',\n",
       " 14: u'lot',\n",
       " 15: u'perform',\n",
       " 16: u'practice',\n",
       " 17: u'pressure',\n",
       " 18: u'professionals',\n",
       " 19: u'say',\n",
       " 20: u'school',\n",
       " 21: u'spends',\n",
       " 22: u'suggest',\n",
       " 23: u'tension',\n",
       " 24: u'time'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the fastest way to swap a dictionary key / value order.  \n",
    "# This is the format gensim LDA expects it's vocabulary.\n",
    "vocab = {v: k for k, v in vectorizer.vocabulary_.iteritems()}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create a token to id mapping with gensim's `corpora.Dictionary`\n",
    "\n",
    "This dictionary class is a more standard way to work with with gensim models. There are a few standard steps we should go through:\n",
    "\n",
    "**7.1. Count the frequency of words.**\n",
    "\n",
    "We can do this easily with the python `defaultdict(int)`, which doesn't require us to already have the key in the dictionary to be able to add to it:\n",
    "\n",
    "```python\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in documents:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Brocolli': 1,\n",
       "             'Health': 1,\n",
       "             'I': 1,\n",
       "             'My': 2,\n",
       "             'Some': 1,\n",
       "             'a': 1,\n",
       "             'and': 1,\n",
       "             'around': 1,\n",
       "             'at': 1,\n",
       "             'baseball': 1,\n",
       "             'better.': 1,\n",
       "             'blood': 1,\n",
       "             'brocolli': 1,\n",
       "             'brocolli,': 1,\n",
       "             'brother': 3,\n",
       "             'but': 2,\n",
       "             'cause': 1,\n",
       "             'do': 1,\n",
       "             'drive': 1,\n",
       "             'driving': 2,\n",
       "             'eat': 1,\n",
       "             'eat.': 1,\n",
       "             'experts': 1,\n",
       "             'feel': 1,\n",
       "             'for': 1,\n",
       "             'good': 3,\n",
       "             'health': 1,\n",
       "             'health.': 1,\n",
       "             'increased': 1,\n",
       "             'is': 2,\n",
       "             'likes': 1,\n",
       "             'lot': 1,\n",
       "             'may': 1,\n",
       "             'mother': 2,\n",
       "             'mother.': 1,\n",
       "             'my': 4,\n",
       "             'never': 1,\n",
       "             'not': 1,\n",
       "             'of': 1,\n",
       "             'often': 1,\n",
       "             'perform': 1,\n",
       "             'practice.': 1,\n",
       "             'pressure': 1,\n",
       "             'pressure.': 1,\n",
       "             'professionals': 1,\n",
       "             'say': 1,\n",
       "             'school,': 1,\n",
       "             'seems': 1,\n",
       "             'spends': 1,\n",
       "             'suggest': 1,\n",
       "             'tension': 1,\n",
       "             'that': 2,\n",
       "             'time': 1,\n",
       "             'to': 6,\n",
       "             'well': 1,\n",
       "             'your': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in documents:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "        \n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2 Remove any words that only appear once, or appear in the stopwords.**\n",
    "\n",
    "Iterate through the documents and only keep useful words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['good', 'My', 'brother', 'good'],\n",
       " ['My', 'mother', 'driving', 'brother'],\n",
       " ['driving'],\n",
       " ['mother', 'brother'],\n",
       " ['good']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [[token for token in text.split() if frequency[token] > 1 and token not in nltk_stops]\n",
    "          for text in documents]\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3 Create the `corpora.Dictionary` object with the retained tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x10ff4f7d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create gensim dictionary object\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4 Use the `dictionary.doc2bow()` function to convert the texts to bag-of-word representations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(3, 1)],\n",
       " [(2, 1), (4, 1)],\n",
       " [(0, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create corpus matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why should we use this process?**\n",
    "\n",
    "The main advantage is that this dictionary object has quick helper functions.\n",
    "\n",
    "However, there are also some major performance advantages if you ever want to save your model to a file, then load it at a later time.  Tokenizations can take a while to be computed, especially when your text files are quite large. You can save these post-computed dictionary items to file, then load them from disk later which is quite a bit faster.  Also, it's possible to add new documents to your corpus without having to re-tokenize your entire set.  This is great for online systems that can take new documents on demmand.  \n",
    "\n",
    "As you work with larger datasets with text, this is a much better way to handle LDA and other Gensim models from a performance point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Set up the LDA model\n",
    "\n",
    "We can create the gensim LDA model object like so:\n",
    "\n",
    "```python\n",
    "lda = models.LdaModel(\n",
    "    # supply our sparse predictor matrix wrapped in a matutils.Sparse2Corpus object\n",
    "    matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    # or alternatively use the corpus object created with the dictionary in the previous frame!\n",
    "    # corpus,\n",
    "    # The number of topics we want:\n",
    "    num_topics  =  3,\n",
    "    # how many passes over the vocabulary:\n",
    "    passes      =  20,\n",
    "    # The id2word vocabulary we made ourselves\n",
    "    id2word     =  vocab\n",
    "    # or use the gensim dictionary object!\n",
    "    # id2word     =  dictionary\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(\n",
    "    matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    num_topics  =  3,\n",
    "    passes      =  20,\n",
    "    id2word     =  vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Look at the topics\n",
    "\n",
    "The model has a `.print_topics` function that accepts the number of topics to print and number of words per topic. The number before the word is the probability of occurance for that word in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.096*\"good\" + 0.096*\"brocolli\" + 0.096*\"eat\" + 0.055*\"suggest\" + 0.055*\"blood\"'),\n",
       " (1,\n",
       "  u'0.093*\"pressure\" + 0.093*\"drive\" + 0.093*\"school\" + 0.093*\"perform\" + 0.093*\"feel\"'),\n",
       " (2,\n",
       "  u'0.115*\"health\" + 0.066*\"driving\" + 0.066*\"baseball\" + 0.066*\"lot\" + 0.066*\"spends\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_topics=3, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Get the topic scores for a document\n",
    "\n",
    "The `.get_document_topics` function accepts a bag-of-words representation for a document and returns the scores for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.64519959976030306), (1, 0.16771135206280566), (2, 0.18708904817689134)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(dictionary.doc2bow(texts[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Label and visualize the topics\n",
    "\n",
    "Lets come up with some high level labels. This is the subjective part of LDA. What do the word probabilties that represent topics mean?  Let's make some up.\n",
    "\n",
    "Plot a heatmap of the topic probabilities for each of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_labels = {\n",
    "   0: \"Family Stress\",\n",
    "   1: \"Driving\",\n",
    "   2: \"Food\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_id  topic        \n",
       "0            Driving          0.280301\n",
       "             Family Stress    0.233094\n",
       "             Food             0.486606\n",
       "1            Driving          0.260823\n",
       "             Family Stress    0.669831\n",
       "             Food             0.069346\n",
       "2            Driving          0.167710\n",
       "             Family Stress    0.645399\n",
       "             Food             0.186891\n",
       "3            Driving          0.112528\n",
       "             Family Stress    0.775387\n",
       "             Food             0.112085\n",
       "4            Driving          0.168522\n",
       "             Family Stress    0.167725\n",
       "             Food             0.663754\n",
       "Name: probability, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics = [lda.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "topic_data = []\n",
    "\n",
    "for document_id, topics in enumerate(doc_topics):\n",
    "    \n",
    "    document_topics = []\n",
    "    \n",
    "    for topic, probability in topics:\n",
    "       \n",
    "        topic_data.append({\n",
    "            'document_id':  document_id,\n",
    "            'topic_id':     topic,\n",
    "            'topic':        topics_labels[topic],\n",
    "            'probability':  probability\n",
    "        })\n",
    "\n",
    "topics_df = pd.DataFrame(topic_data)\n",
    "topics_df.pivot_table(values=\"probability\", index=[\"document_id\", \"topic\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11092eb50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAHVCAYAAAB4wWYZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11092ec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "doc_topics = [lda.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "doc_topic_probabilities = []\n",
    "\n",
    "for document in doc_topics:\n",
    "    \n",
    "    single_document = []\n",
    "    \n",
    "    for topic, probablity in document:\n",
    "        \n",
    "        single_document.append(probablity)\n",
    "        \n",
    "    doc_topic_probabilities.append(single_document)\n",
    "    \n",
    "docs_topics = pd.DataFrame(doc_topic_probabilities)\n",
    "sns.heatmap(docs_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Fit an LDA model with sklearn\n",
    "\n",
    "Sklearn's LDA model is in the decomposition submodule:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "```\n",
    "\n",
    "One of the greatest benefits of the sklearn implementation is that it comes with the familiar `.fit()`, `.transform()` and `.fit_transform()` methods.\n",
    "\n",
    "**12.1 Initialize and fit an sklearn LDA with `n_topics=3` on our output from the CountVectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juan/.envs/dsi-ldn-1/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_jobs=1, n_topics=3, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=3)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.2 Print out the topic-word distributions using the `.components_` attribute.**\n",
    "\n",
    "Each row of this matrix represents a topic, and the columns are the words. (These are not probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49781209,  1.27402379,  1.2477801 ,  0.46861289,  1.25419388,\n",
       "         1.23776168,  1.27096774,  0.44660319,  1.26271868,  1.24066622,\n",
       "         0.46841576,  1.25405299,  1.25224773,  0.47253947,  0.45540774,\n",
       "         1.27858887,  0.4732278 ,  2.07597736,  0.44305942,  0.45805885,\n",
       "         1.27648248,  0.46891448,  1.28988962,  1.29446121,  0.44719948],\n",
       "       [ 1.25283772,  0.45369184,  0.47811041,  2.88247375,  0.49607562,\n",
       "         0.4593824 ,  1.2670328 ,  2.06309699,  0.4939157 ,  0.45533119,\n",
       "         2.8892691 ,  2.06147274,  0.43482384,  1.26231546,  1.27976664,\n",
       "         0.45127359,  1.27096809,  0.44689553,  1.27101578,  1.25085405,\n",
       "         0.46250359,  1.22284392,  0.43505286,  0.45144861,  1.30027764],\n",
       "       [ 0.49182497,  0.46772861,  0.4566583 ,  0.46388457,  0.47311436,\n",
       "         0.45626365,  0.45140745,  0.45140178,  0.46287911,  0.52423369,\n",
       "         0.43967574,  0.47941746,  0.45038515,  0.49203083,  0.45422785,\n",
       "         0.46963254,  0.45805213,  0.46048301,  0.50225779,  0.49313298,\n",
       "         0.43106518,  0.48713293,  0.46155655,  0.44292379,  0.47827494]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.3 Use the `.transform()` method to convert the matrix into the topic scores.**\n",
    "\n",
    "These are the document-topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04211932,  0.91522674,  0.04265395],\n",
       "       [ 0.05011566,  0.89958578,  0.05029855],\n",
       "       [ 0.92988237,  0.03539582,  0.03472181],\n",
       "       [ 0.90193518,  0.04836501,  0.04969981],\n",
       "       [ 0.04953357,  0.90117275,  0.04929368]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Further steps\n",
    "\n",
    "This has been a very basic example.  LDA typically doesn't perform well on very small datasets.  You should try to see how it behaves on your own using a larger text dataset.  Keep in mind: finding the optimal number of topics can be tricky and subjective.\n",
    "\n",
    "**Generally, you should consider:**\n",
    "- How well topics are applied to documents overall\n",
    "- The strength of topics overall, to all documents\n",
    "- Improving preprocessing such as stopword removal\n",
    "- Building a nice web interface to explore your documents (see: [LDAExplorer](https://github.com/dyerrington/LDAExplorer), and [pyLDAvis](https://github.com/bmabey/pyLDAvis/blob/master/README.rst))\n",
    "\n",
    "These general guidelines should help you tune your hyperparameter **K** for number of topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
